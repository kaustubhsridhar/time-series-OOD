#!/usr/bin/env python3
#input: script takes in the latent variable csv generated by the latent unit generator script.
#output: Average KL-divergence of each latent variables across all the scenes in a partition. The output stored as csv file. 
#Libraries
import os
import matplotlib.pyplot as plt
import numpy as np
import csv
from sklearn import preprocessing
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN
from sklearn.neighbors import KNeighborsClassifier
from scipy import stats
from math import log2
import math
from scipy.stats import norm
from statistics import mean
from scipy.stats import wasserstein_distance
seed = 7
np.random.seed(seed)

def kl_divergence(Q, P):
     epsilon = 0.00001
     P = P+epsilon
     Q = Q+epsilon

     divergence = np.sum(P*np.log(P/Q))
     return divergence

    #Extracts the mean and logvar from the csv files.
    #computes the dissimilarity from unit gaussian using kl-divergence.
    #Averages it across the entire time-series data for each latent unit.
def KL_computer(Fadress,latentsize):
    train_distribution = []
    a = []
    m = []

    for i in range(2):
        train_distribution.append([])
        a.append([])

    for i in range(2):
        for j in range(latentsize):
            train_distribution[i].append([])


    for x in range(2):
        with open(Fadress, 'rt') as csvfile:
              reader = csv.reader(csvfile)
              for row in reader:
                  data = row[x].strip().split(',')
                  data[0] = data[0][1:]
                  data[len(data)-1]=data[len(data)-1][:-1]
                  data = np.array(data)
                  for y in range (latentsize):
                      train_distribution[x][y].append(float(data[y]))
    kl=[]
    for z in range(latentsize):
        avg_klloss = []
        kl_val = 0.0
        for k in range(len(train_distribution[0][0])):
            mean = train_distribution[0][z][k]
            logvar = train_distribution[1][z][k]
            sd = math.sqrt(math.exp(logvar))
            x = np.arange(-10, 10, 0.001)
            p = norm.pdf(x, 0, 1)  # Normal Curve
            sum_p = np.sum(p)
            p[:] = [y / sum_p for y in p]
            q = norm.pdf(x, mean, sd) #
            sum_q = np.sum(q)
            q[:] = [z / sum_q for z in q]
            klloss = kl_divergence(q, p)
            #klloss = wasserstein_distance(q,p)

            avg_klloss.append(klloss)
        kl.append(avg_klloss)

    return kl

if __name__ == '__main__':
        models = ["30_1.1"] #"30_1.0",
        runs = ["precipitation","brightness"]
        #latentsize=30
        monitor_list = []
        csv_list = ["s0","p25","p50"]
        csv_list1 = ["s0","b25","b50"]
        #csv_lists = [csv_list,csv_list1]
        for model in models:
            if(model == "30_1.0" or model == "30_1.2" or model == "30_1.4" or model == "30_1.1"):
                latentsize = 30
            if(model == "40_1.0" or model == "40_1.5"):
                latentsize = 40
            for run in runs:
                all_comparisons = []
                x = 0
                if run == "precipitation":
                    csv_list = csv_list
                elif run == "brightness":
                    csv_list = csv_list1
                path = "/home/scope/Carla/CARLA_0.9.6/PythonAPI/TCPS-results/Latent-extraction/" + model + '/'
                for elements in csv_list:
                    print('-----------------------Run%d-----------------------'%x)
                    csv_file = elements + '.csv'
                    Faddress = path + csv_file
                    kl_value = KL_computer(Faddress,latentsize)
                    kl_comparison = []
                    Higgins = []
                    if(len(kl_value[0])%2!=0):
                        kl_value[0].pop(len(kl_value[0])-1)
                    for j in range (1,len(kl_value[0]),1):
                        Final_kl = []
                        for k in range(latentsize):
                            kl = abs(kl_value[k][j]- kl_value[k][j-1])
                            Final_kl.append(kl)
                        kl_comparison.append(Final_kl)
                    print(len(kl_comparison))
                    for i in range(30):
                        val = 0.0
                        for k in range(len(kl_comparison)):
                            val += kl_comparison[k][i]
                        val = val/len(kl_comparison)
                        Higgins.append(val)
                    print(Higgins)
                    x+=1
                    all_comparisons.append(Higgins)

                with open(path + run + '-comparison-new.csv', 'a') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerows(all_comparisons)
